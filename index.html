<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="We propose a novel phonetic context-aware loss to generate smoother and more realistic 3D facial animations by explicitly modeling coarticulation.">
  <meta property="og:title" content="Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation"/>
  <meta property="og:description" content="We propose a novel phonetic context-aware loss to generate smoother and more realistic 3D facial animations by explicitly modeling coarticulation."/>
  <meta property="og:url" content="https://kimhyungkyu-1208.github.io/interspeech25/"/> <meta property="og:image" content="static/images/interspeech25_phonetic.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation">
  <meta name="twitter:description" content="We propose a novel phonetic context-aware loss to generate smoother and more realistic 3D facial animations by explicitly modeling coarticulation.">
  <meta name="twitter:image" content="static/images/interspeech25_phonetic.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Speech-driven 3D Facial Animation, Phonetic Context, Coarticulation, 3D Face, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Phonetic Context-Aware 3D Facial Animation</title> <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <span class="author-block"><a href="#" target="_blank">Hyung Kyu Kim</a></span>
                <span class="author-block"><a href="#" target="_blank">Hak Gu Kim</a></span>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"></span><br>
                    <span class="author-block">Interspeech 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="static/pdfs/interspeech25_1692-submssion.pdf">
                         <span class="link-block">
                        <a href="#" target="_blank"
                        class="external-link button is-normal is-rounded is-dark"> <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                    <a href="https://github.com/kimhyungkyu-1208/interspeech25" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark"> <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/interspeech25_phonetic.png" alt="Visual comparison of generated visemes"/>
      <h2 class="subtitle has-text-centered">
        We propose a novel phonetic context-aware loss which explicitly models the influence of phonetic context on viseme transitions, ensuring smoother and perceptually consistent animations.
      </h2>
    </div>
  </div>
</section>
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Speech-driven 3D facial animation aims to generate realistic facial movements synchronized with audio. Traditional methods primarily minimize reconstruction loss by aligning each frame with ground-truth. However, this frame-wise approach often fails to capture the continuity of facial motion, leading to jittery and unnatural outputs due to coarticulation. To address this, we propose a novel phonetic context-aware loss, which explicitly models the influence of phonetic context on viseme transitions. By incorporating a viseme coarticulation weight, we assigns adaptive importance to facial movements based on their dynamic changes over time, ensuring smoother and perceptually consistent animations. Extensive experiments demonstrate that replacing the conventional reconstruction loss with ours improves both quantitative metrics and visual quality. It highlights the importance of explicitly modeling phonetic context-dependent visemes in synthesizing natural speech-driven 3D facial animation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{kim2025learning,
  title={Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation},
  author={Hyung Kyu Kim and Hak Gu Kim},
  booktitle={Proc. INTERSPEECH},
  year={2025},
  organization={ISCA}
}</code></pre>
    </div>
</section>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
